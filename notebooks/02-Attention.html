<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christof Claessens">
<meta name="dcterms.date" content="2025-06-01">

<title>Attention mechanics – TinyLM - language model learnings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-0836c12f657ee7ab470fa997119f6577.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/01-Tokenizing.html">Notebooks</a></li><li class="breadcrumb-item"><a href="../notebooks/02-Attention.html">Attention</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">TinyLM - language model learnings</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../theory/softmax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Softmax Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../theory/cross-entropy-loss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cross Entropy Loss</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/01-Tokenizing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-Attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Attention</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link active" data-scroll-target="#self-attention"><span class="header-section-number">1</span> Self-Attention</a>
  <ul class="collapse">
  <li><a href="#simplified-self-attention" id="toc-simplified-self-attention" class="nav-link" data-scroll-target="#simplified-self-attention"><span class="header-section-number">1.1</span> Simplified Self-Attention</a>
  <ul class="collapse">
  <li><a href="#attention-scores" id="toc-attention-scores" class="nav-link" data-scroll-target="#attention-scores"><span class="header-section-number">1.1.1</span> Attention scores</a></li>
  <li><a href="#normalizing" id="toc-normalizing" class="nav-link" data-scroll-target="#normalizing"><span class="header-section-number">1.1.2</span> Normalizing</a></li>
  <li><a href="#context-vector" id="toc-context-vector" class="nav-link" data-scroll-target="#context-vector"><span class="header-section-number">1.1.3</span> Context Vector</a></li>
  <li><a href="#all-context-vectors" id="toc-all-context-vectors" class="nav-link" data-scroll-target="#all-context-vectors"><span class="header-section-number">1.1.4</span> All context vectors</a></li>
  </ul></li>
  <li><a href="#self-attention-with-trainable-weights" id="toc-self-attention-with-trainable-weights" class="nav-link" data-scroll-target="#self-attention-with-trainable-weights"><span class="header-section-number">1.2</span> Self-Attention with trainable weights</a>
  <ul class="collapse">
  <li><a href="#single-context-vector-1" id="toc-single-context-vector-1" class="nav-link" data-scroll-target="#single-context-vector-1"><span class="header-section-number">1.2.1</span> Single context vector (1)</a></li>
  <li><a href="#why-scaling" id="toc-why-scaling" class="nav-link" data-scroll-target="#why-scaling"><span class="header-section-number">1.2.2</span> Why scaling?</a></li>
  <li><a href="#single-context-vector-2" id="toc-single-context-vector-2" class="nav-link" data-scroll-target="#single-context-vector-2"><span class="header-section-number">1.2.3</span> Single context vector (2)</a></li>
  <li><a href="#all-context-vectors-1" id="toc-all-context-vectors-1" class="nav-link" data-scroll-target="#all-context-vectors-1"><span class="header-section-number">1.2.4</span> All context vectors</a></li>
  </ul></li>
  <li><a href="#causal-self-attention" id="toc-causal-self-attention" class="nav-link" data-scroll-target="#causal-self-attention"><span class="header-section-number">1.3</span> Causal self-attention</a>
  <ul class="collapse">
  <li><a href="#naive-self-attention" id="toc-naive-self-attention" class="nav-link" data-scroll-target="#naive-self-attention"><span class="header-section-number">1.3.1</span> Naive self-attention</a></li>
  <li><a href="#optimized-self-attention" id="toc-optimized-self-attention" class="nav-link" data-scroll-target="#optimized-self-attention"><span class="header-section-number">1.3.2</span> Optimized self-attention</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/01-Tokenizing.html">Notebooks</a></li><li class="breadcrumb-item"><a href="../notebooks/02-Attention.html">Attention</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Attention mechanics</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christof Claessens </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>In the previous notebook we prepared and massaged our text into input vectors that encode both tokens as well as their positions. It’s these input vectors that will be handled by the rest of the model.</p>
<section id="self-attention" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Self-Attention</h1>
<p>Self-Attention is a technique whereby each position in the input sequence can consider the relevancy of each other position in the same sequence when the representation for the sequence is being computed. (Traditional “attention”, as opposed to self-attention look at relations between two different sequences, input- and output, as opposed as using a single sequence.)</p>
<section id="simplified-self-attention" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="simplified-self-attention"><span class="header-section-number">1.1</span> Simplified Self-Attention</h2>
<p>Let’s take a simple sentence: “I am learning this”</p>
<p>This is our input sequencen let’s call it <span class="math inline">\(x\)</span> with 4 tokens: <span class="math inline">\(x^{(1)}\)</span>, <span class="math inline">\(x^{(2)}\)</span>, <span class="math inline">\(x^{(3)}\)</span> and <span class="math inline">\(x^{(4)}\)</span>.</p>
<p>In general if our context length is <span class="math inline">\(T\)</span> then we have <span class="math inline">\(x^{(1)} \ldots x^{(T)}\)</span></p>
<p>Each <span class="math inline">\(x^{(i)}\)</span> is a <span class="math inline">\(d\)</span>-dimensional embedding vector representing a token.</p>
<p>Now we will calculate a context vector <span class="math inline">\(z^{i}\)</span> for each <span class="math inline">\(x^{i}\)</span>. This vector will contain information from all vectors <span class="math inline">\(x^{(1)} \ldots x^{(T)}\)</span></p>
<section id="attention-scores" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="attention-scores"><span class="header-section-number">1.1.1</span> Attention scores</h3>
<p>So, for example, for <span class="math inline">\(x^{(3)}\)</span> we’ll calculate a context vector <span class="math inline">\(z^{(3)}\)</span>. We’ll call <span class="math inline">\(x^{(3)}\)</span> our “query” vector and for this query we’ll calculate an attention score for each of the tokens in our sentence: for example <span class="math inline">\(w_{31}\)</span> for the attention score between our query (the 3rd token) and the first token. In this way we’ll have: <span class="math inline">\(w_{31}\)</span>, <span class="math inline">\(w_{32}\)</span>, <span class="math inline">\(w_{33}\)</span> and <span class="math inline">\(w_{34}\)</span></p>
<p>The attention score <span class="math inline">\(w_{31}\)</span> is the dot product from the third token with the first token. So if: <span class="math display">\[ x^{(3)} = [0.2,0.7,0.9] \text{  (representing the token "learning")} \]</span> <span class="math display">\[ x^{(1)} = [0.8,0.9,0.3] \text{  (representing the token "I")} \]</span></p>
<p>Then:</p>
<p><span class="math display">\[w_{(31)} = x^{(3)} \cdot x^{(1)} = [(.2*.8) + (.7*.9) + (.9 * .3)] = 1.06 \]</span></p>
<p>We’ll do this between our query <span class="math inline">\(x^{(3)}\)</span> and each of our <span class="math inline">\(x^{(1)},x^{(2)},x^{(3)},x^{(4)}\)</span> which result in an attention vector <span class="math inline">\(w_3 = [ w_{(31)},w_{(32)},w_{(33)},w_{(34)} ]\)</span></p>
<p>Now in python, using an input vector of our 4 tokens, each with an embedding dimension of 3:</p>
<div id="87225baa" class="cell" data-execution_count="59">
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936]])</code></pre>
</div>
</div>
<p>We’ll calculate the attention vector for <span class="math inline">\(x^{(3)}\)</span> as our query:</p>
<div id="df259370" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> inputs[<span class="dv">2</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>attention_score_for_x3 <span class="op">=</span> torch.empty((<span class="dv">4</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, xi <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the attention score for x3i</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    attention_score_for_x3[i] <span class="op">=</span> torch.dot(xi, query)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_score_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.3127, 1.1213, 1.5807, 1.3343])</code></pre>
</div>
</div>
</section>
<section id="normalizing" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="normalizing"><span class="header-section-number">1.1.2</span> Normalizing</h3>
<p>Now that we’ve calculated the attention vector <span class="math inline">\(w_3\)</span> for query <span class="math inline">\(x^{(3)}\)</span>, we have a vector with a number for every token. These numbers however are not normalized between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, which is what we’d really like. For this, we’ll use <a href="../theory/cross-entropy-loss.html">Cross-Entropy Loss</a>, which will make sure the numbers in our attention vector add up to one and each are between <span class="math inline">\(0 \ldots 1\)</span></p>
<div id="26d7f4f9" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>attention_weights_for_x3 <span class="op">=</span> torch.softmax(attention_score_for_x3, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_weights_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.2407, 0.1987, 0.3146, 0.2459])</code></pre>
</div>
</div>
</section>
<section id="context-vector" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="context-vector"><span class="header-section-number">1.1.3</span> Context Vector</h3>
<p>So now that we have our normalized attention weights for a single query <span class="math inline">\(x^{(3)}\)</span> we can calculate the full context vector that corresponds to <span class="math inline">\(x^{(3)}\)</span></p>
<p>Our attention weights from the previous step were: <span class="math inline">\([.2, .1, .3, .2]\)</span> or more in general, a vector <span class="math inline">\([\alpha _{31}, \alpha _{32}, \alpha _{33}, \alpha _{34}]\)</span></p>
<p>To calculate the context vector for <span class="math inline">\(x^{(3)}\)</span> we’ll take each <span class="math inline">\(\alpha _{3i}\)</span> and multiply that by <span class="math inline">\(x^{(i)}\)</span>. Then we’ll add up all those vectors.</p>
<div id="68002bda" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> inputs[<span class="dv">2</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>context_vector_for_x3 <span class="op">=</span> torch.zeros((query.shape)) <span class="co"># dimension of query (3 in this case)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, xi <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    context_vector_for_x3 <span class="op">+=</span> attention_weights_for_x3[i] <span class="op">*</span> xi <span class="co"># z3</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vector_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.5165, 0.7774, 0.6536])</code></pre>
</div>
</div>
</section>
<section id="all-context-vectors" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="all-context-vectors"><span class="header-section-number">1.1.4</span> All context vectors</h3>
<p>What we did so far is to look at how to calculate a single context vector <span class="math inline">\(z^{(3)}\)</span>, for a single token <span class="math inline">\(x^{(3)}\)</span> in our input sequence. We’ll need to make this more scalable and figure out a way to calculate <span class="math inline">\(z\)</span> for all tokens in our input sequence.</p>
<p>This means, what we have done for <span class="math inline">\(x^{(3)}\)</span> we need to do for all inputs:</p>
<div id="028b9ac1" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>attention_scores_manual <span class="op">=</span> torch.empty((<span class="dv">4</span>, <span class="dv">4</span>))  <span class="co"># for each query of the 4 inputs, calculate 4 attention scores</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, query <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, xi <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        attention_scores_manual[i][j] <span class="op">=</span> torch.dot(query, xi)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention scores:"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_scores_manual)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"for comparison, the attention scores for x3 as query:"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_score_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention scores:
tensor([[1.7622, 1.4337, 1.3127, 1.1999],
        [1.4337, 1.4338, 1.1213, 0.8494],
        [1.3127, 1.1213, 1.5807, 1.3343],
        [1.1999, 0.8494, 1.3343, 1.2435]])
for comparison, the attention scores for x3 as query:
tensor([1.3127, 1.1213, 1.5807, 1.3343])</code></pre>
</div>
</div>
<p>Using <code>for</code> loops however is slow and can’t be optimized using CUDA, so let’s find a way to do the same, but to use pure tensor calculations instead. Our inputs look like the below, with 4 tokens, each with a dimension of 3</p>
<div id="b4de671a" class="cell" data-execution_count="64">
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936]])</code></pre>
</div>
</div>
<p>We can transpose this vector, so it looks like:</p>
<div id="3ff2e719" class="cell" data-execution_count="65">
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.8823, 0.9593, 0.2566, 0.1332],
        [0.9150, 0.3904, 0.7936, 0.9346],
        [0.3829, 0.6009, 0.9408, 0.5936]])</code></pre>
</div>
</div>
<p>If we now take multiply these two matrices, we get:</p>
<div id="dbb777d1" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>attention_scores <span class="op">=</span> inputs <span class="op">@</span> inputs.T  <span class="co"># matrix multiplication</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs <span class="op">@</span> inputs.T) <span class="co"># matrix multiplication</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"for comparison, the attention scores for x3 as query:"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_score_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1.7622, 1.4337, 1.3127, 1.1999],
        [1.4337, 1.4338, 1.1213, 0.8494],
        [1.3127, 1.1213, 1.5807, 1.3343],
        [1.1999, 0.8494, 1.3343, 1.2435]])
for comparison, the attention scores for x3 as query:
tensor([1.3127, 1.1213, 1.5807, 1.3343])</code></pre>
</div>
</div>
<p>These are just attention scores, not yet attention weights, so lets normalize them:</p>
<div id="4ac7edec" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> torch.softmax(attention_scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># normalize the attention scores</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention weights:"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_weights)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"for comparison, the attention weights for x3 as query:"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_weights_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention weights:
tensor([[0.3415, 0.2459, 0.2179, 0.1946],
        [0.3040, 0.3040, 0.2225, 0.1695],
        [0.2407, 0.1987, 0.3146, 0.2459],
        [0.2569, 0.1809, 0.2938, 0.2683]])
for comparison, the attention weights for x3 as query:
tensor([0.2407, 0.1987, 0.3146, 0.2459])</code></pre>
</div>
</div>
<p>From here we can calculate our context vectors:</p>
<div id="ba50a5fd" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>all_context_vectors <span class="op">=</span> attention_weights <span class="op">@</span> inputs  <span class="co"># matrix multiplication</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"context vectors for all inputs:"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(all_context_vectors)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"for comparison, the context vector for x3:"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vector_for_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>context vectors for all inputs:
tensor([[0.6191, 0.7634, 0.5991],
        [0.6395, 0.7318, 0.6090],
        [0.5165, 0.7774, 0.6536],
        [0.5113, 0.7897, 0.6428]])
for comparison, the context vector for x3:
tensor([0.5165, 0.7774, 0.6536])</code></pre>
</div>
</div>
</section>
</section>
<section id="self-attention-with-trainable-weights" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="self-attention-with-trainable-weights"><span class="header-section-number">1.2</span> Self-Attention with trainable weights</h2>
<p>From here we’ll expand to what is called “scaled dot-product attention”. Also here we’ll want to calculate context vectors (one for each of our input tokens) as a weighted sum over (some abstraction) of the inputs. There are some differences with what we’ve done so far though:</p>
<ul>
<li>instead of taking directly <span class="math inline">\(q^{(i)} = x^{(i)}\)</span> as the query vector, we’ll use a projection of <span class="math inline">\(x^{(i)}\)</span>. We do this using a trainable weight matrix <span class="math inline">\(W_q\)</span>, used to calculate our query</li>
<li>instead of taking directly the dot product of <span class="math inline">\(q^{(i)}\)</span> and each <span class="math inline">\(x^{(i)}\)</span> to calculate our attention scores and weights, we’ll do the dot product with a projection of <span class="math inline">\(x^{(i)}\)</span>. We do this using a trainable weight matrix <span class="math inline">\(W_k\)</span>, used to calculate our keys.</li>
<li>instead of calculating the weighted average using these scores with each <span class="math inline">\(x^{(i)}\)</span>, we’ll do this with a projection of <span class="math inline">\(x^{(i)}\)</span>. We do this using a trainable weight matrix <span class="math inline">\(W_v\)</span>, used to calculate our values.</li>
</ul>
<p>We’ll have trainable weight matrixes: <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, <span class="math inline">\(W_v\)</span> so that the model, when trained can learn to use these to project respectively the query, key and value vectors.</p>
<section id="single-context-vector-1" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="single-context-vector-1"><span class="header-section-number">1.2.1</span> Single context vector (1)</h3>
<p>Our token embedding vectors are of a certain dimension <code>embedding_dim = 3</code> in our example. We can choose to project these into another dimension, from 3 to 5 for example. (This is not usually done however, it’s likely kept the same.) For illustration purposes, let’s go with 5 here:</p>
<div id="2db615f5" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>d_in <span class="op">=</span> embedding_dim</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>d_out <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>W_query_1 <span class="op">=</span> torch.rand((d_in, d_out))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>W_key_1 <span class="op">=</span> torch.rand((d_in, d_out))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>W_value_1 <span class="op">=</span> torch.rand((d_in, d_out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now use <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, <span class="math inline">\(W_v\)</span> to project an embedding vector from it’s normal dimension into a dimension of 5. Let’s use <span class="math inline">\(x^{(3)}\)</span> as an example for our query:</p>
<div id="6980ee82" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>x3 <span class="op">=</span> inputs[<span class="dv">2</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"x3 input:"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x3)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"W_query:"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W_query_1)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> x3 <span class="op">@</span> W_query_1</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"projected query:"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x3 input:
tensor([0.2566, 0.7936, 0.9408])
W_query:
tensor([[0.8823, 0.9150],
        [0.3829, 0.9593],
        [0.3904, 0.6009]])
projected query:
tensor([0.8975, 1.5614])</code></pre>
</div>
</div>
<p>Instead of defining our projection matrices like above, we’ll define them as pytorch parameters:</p>
<div id="5938b6c2" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>W_query <span class="op">=</span> torch.nn.Parameter(W_query_1, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>W_key <span class="op">=</span> torch.nn.Parameter(W_key_1, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>W_value <span class="op">=</span> torch.nn.Parameter(W_value_1, requires_grad<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s calculate our key and value vectors for every input token in our sentence “I am learning this”: <span class="math inline">\(x^{(1)}\)</span>: “I”, <span class="math inline">\(x^{(2)}\)</span>: “am”, <span class="math inline">\(x^{(3)}\)</span>: “learning”, <span class="math inline">\(x^{(4)}\)</span>: “this”,</p>
<div id="b833c4d4" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> inputs <span class="op">@</span> W_value</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"values:"</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(values)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> inputs <span class="op">@</span> W_key</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"keys:"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(keys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>values:
tensor([[1.7842, 1.1135],
        [1.6554, 1.0571],
        [1.6442, 1.0264],
        [1.3340, 0.8176]])
keys:
tensor([[1.4450, 1.0493],
        [1.1750, 1.1700],
        [1.6917, 0.8678],
        [1.4682, 0.5825]])</code></pre>
</div>
</div>
<p>Now we can calculate the attention scores, for our query vector that got projected from <span class="math inline">\(x^{(3)}\)</span></p>
<div id="55f57929" class="cell" data-execution_count="73">
<div class="cell-output cell-output-stdout">
<pre><code>query projection from x3:
tensor([0.8975, 1.5614])</code></pre>
</div>
</div>
<p>The attention score <span class="math inline">\(w_{31}\)</span> is:</p>
<div id="51c04b8f" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> inputs[<span class="dv">0</span>]</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"x1 input:"</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x1)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"key projection from x1: "</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> x1 <span class="op">@</span> W_key</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(key)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"... which is the same as: "</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(keys[<span class="dv">0</span>])  <span class="co"># keys[0] is the key for x1</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention score between our query and x1's projected key:"</span>)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>attention_score_x3_x1 <span class="op">=</span> query.dot(key)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_score_x3_x1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x1 input:
tensor([0.8823, 0.9150, 0.3829])
key projection from x1: 
tensor([1.4450, 1.0493])
... which is the same as: 
tensor([1.4450, 1.0493])
attention score between our query and x1's projected key:
tensor(2.9354)</code></pre>
</div>
</div>
<p>In the same way as we got the attention score between the query and x1, we can get all attention scores for our query like this:</p>
<div id="c8599d0e" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>attention_scores_x3_as_query <span class="op">=</span> query <span class="op">@</span> keys.T</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention scores for x3 as query:"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_scores_x3_as_query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention scores for x3 as query:
tensor([2.9354, 2.8816, 2.8733, 2.2273])</code></pre>
</div>
</div>
<p>As is shown above, for our chosen query, we’ll end up with 4 attention scores: one for each input token. Like before, we’ll want to normalize our attention scores into attention weights but instead of doing a pure softmax function, we’ll first scale the attention scores by dividing them by the square root of the dimension of our projected keys (2 in this case).</p>
<div id="ee0dbd7a" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"dimension of our projected key: "</span>, d_out)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>scaled_attention_weights_x3_as_query <span class="op">=</span> <span class="op">\</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  torch.softmax(attention_scores_x3_as_query <span class="op">/</span> d_out<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scaled attention weights for x3 as query:"</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scaled_attention_weights_x3_as_query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dimension of our projected key:  2
scaled attention weights for x3 as query:
tensor([0.2836, 0.2730, 0.2714, 0.1719])</code></pre>
</div>
</div>
<p>We don’t have our full context vector yet. For that we’ll still need to make a weighted combination of our projected value vectors. Before we do so, let’s digress a little on the need for scaling the softmax.</p>
</section>
<section id="why-scaling" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="why-scaling"><span class="header-section-number">1.2.2</span> Why scaling?</h3>
<p>As the dimension for our projected key vector grows, the dot products can become large numbers. An example:</p>
<div id="d45e5e84" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>key_dim <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"dimension of the key: "</span>, key_dim)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>small_proj_key_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>large_proj_key_dim <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>test_token_embeddings <span class="op">=</span> torch.rand((<span class="dv">4</span>, key_dim)) <span class="co"># 4 tokens, each with key_dim features</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"test token embeddings for 4 token:"</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_token_embeddings)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>test_query  <span class="op">=</span> test_token_embeddings[<span class="dv">2</span>]  <span class="co"># let's take the 3rd token as query</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"test query:"</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_query)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>W_k_for_small_output_dim <span class="op">=</span> torch.rand(key_dim, small_proj_key_dim)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>W_q_for_small_output_dim <span class="op">=</span> torch.rand(key_dim, small_proj_key_dim)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>W_k_for_large_output_dim <span class="op">=</span> torch.rand(key_dim, large_proj_key_dim)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>W_q_for_large_output_dim <span class="op">=</span> torch.rand(key_dim, large_proj_key_dim)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>projected_small_keys <span class="op">=</span> test_token_embeddings <span class="op">@</span> W_k_for_small_output_dim</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>projected_small_query <span class="op">=</span> test_query <span class="op">@</span> W_q_for_small_output_dim</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>projected_large_keys <span class="op">=</span> test_token_embeddings <span class="op">@</span> W_k_for_large_output_dim</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>projected_large_query <span class="op">=</span> test_query <span class="op">@</span> W_q_for_large_output_dim</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"projected small query:"</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(projected_small_query)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"projected small keys:"</span>)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(projected_small_keys)</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"projected large query:"</span>)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(projected_large_query)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"projected large keys:"</span>)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(projected_large_keys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dimension of the key:  3
test token embeddings for 4 token:
tensor([[0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317],
        [0.1053, 0.2695, 0.3588],
        [0.1994, 0.5472, 0.0062]])
test query:
tensor([0.1053, 0.2695, 0.3588])
projected small query:
tensor([0.4380, 0.3182])
projected small keys:
tensor([[0.9006, 0.6041],
        [0.9639, 0.8792],
        [0.4601, 0.4554],
        [0.6766, 0.3391]])
projected large query:
tensor([0.1878, 0.3446, 0.4501, 0.3379, 0.3108, 0.1170, 0.1112, 0.4571, 0.4659,
        0.4757, 0.2060, 0.3174, 0.3258, 0.3598, 0.2669, 0.5505, 0.6264, 0.1989,
        0.5678, 0.3476, 0.3972, 0.4965, 0.3051, 0.4656, 0.3402, 0.3913, 0.4238,
        0.1793, 0.2759, 0.1342, 0.4060, 0.4709, 0.5078, 0.2881, 0.1066, 0.2342,
        0.5123, 0.1315, 0.3327, 0.5920, 0.3685, 0.3478, 0.2677, 0.1843, 0.1589,
        0.1085, 0.1547, 0.3190, 0.4401, 0.2056, 0.4259, 0.4452, 0.4818, 0.3970,
        0.4401, 0.4995, 0.1077, 0.2400, 0.3922, 0.2375, 0.1725, 0.5728, 0.4268,
        0.3151])
projected large keys:
tensor([[0.3035, 0.5216, 0.4596, 0.2893, 0.8955, 0.6188, 0.6455, 0.2561, 0.3482,
         0.2990, 0.5600, 0.4848, 0.5095, 0.4012, 0.6952, 0.5336, 0.7148, 0.2508,
         0.3392, 0.3222, 0.4917, 0.2481, 0.9719, 0.6077, 0.9490, 0.5704, 0.2386,
         0.7114, 0.6478, 0.5438, 0.3272, 0.1102, 0.6529, 0.4615, 0.6076, 0.2971,
         0.7800, 0.3415, 0.4149, 0.6020, 0.9005, 0.8435, 0.8293, 0.2204, 0.7158,
         0.4737, 0.4997, 0.3527, 0.9515, 0.2662, 0.7417, 0.8811, 0.5258, 0.3140,
         0.8549, 0.6960, 0.6771, 0.2255, 0.5432, 0.4350, 0.3110, 0.8551, 0.7196,
         0.4307],
        [0.5414, 0.8804, 1.0148, 0.4877, 0.8235, 0.7101, 0.6321, 0.6603, 0.7909,
         0.2588, 0.7355, 1.0503, 0.8600, 0.6394, 1.0722, 0.9444, 1.1029, 0.4545,
         0.5869, 0.5922, 0.7288, 0.4679, 1.2429, 1.0669, 1.0546, 0.6073, 0.5337,
         0.7313, 1.0426, 0.9182, 0.4964, 0.3018, 0.9588, 0.7419, 0.6737, 0.5018,
         0.7198, 0.4869, 0.5098, 0.8060, 1.2439, 1.0076, 0.6866, 0.3648, 0.8830,
         0.9519, 0.7175, 0.6335, 1.1389, 0.4614, 0.7267, 1.2907, 0.4841, 0.5035,
         0.7083, 1.0215, 1.0409, 0.5922, 0.4310, 0.4074, 0.3501, 1.0624, 1.0229,
         0.9020],
        [0.1838, 0.3311, 0.3795, 0.2121, 0.4295, 0.3402, 0.3010, 0.2730, 0.3651,
         0.1327, 0.3892, 0.3841, 0.3565, 0.2441, 0.4775, 0.4100, 0.4658, 0.2066,
         0.2301, 0.2715, 0.3052, 0.1889, 0.6106, 0.4833, 0.5087, 0.3374, 0.2239,
         0.4179, 0.4202, 0.3830, 0.1427, 0.1232, 0.5042, 0.3661, 0.3749, 0.1694,
         0.3759, 0.1509, 0.2579, 0.4196, 0.5787, 0.5371, 0.3629, 0.1396, 0.4025,
         0.4415, 0.3514, 0.2355, 0.5749, 0.1862, 0.3541, 0.5809, 0.2908, 0.2299,
         0.3590, 0.4987, 0.5014, 0.2625, 0.2681, 0.1827, 0.1524, 0.5781, 0.4842,
         0.4073],
        [0.1846, 0.3146, 0.1962, 0.1578, 0.7056, 0.4448, 0.5090, 0.0615, 0.1004,
         0.2439, 0.3434, 0.2190, 0.2877, 0.2528, 0.4102, 0.2744, 0.4366, 0.1196,
         0.1940, 0.1490, 0.3129, 0.1252, 0.6383, 0.3039, 0.6941, 0.4018, 0.0853,
         0.5061, 0.3915, 0.3056, 0.2424, 0.0210, 0.3584, 0.2398, 0.4157, 0.1903,
         0.6133, 0.2556, 0.2766, 0.3666, 0.5725, 0.5584, 0.6834, 0.1339, 0.5017,
         0.1842, 0.2965, 0.2012, 0.6479, 0.1490, 0.5800, 0.5425, 0.3916, 0.1746,
         0.7133, 0.4049, 0.3785, 0.0399, 0.4320, 0.3572, 0.2354, 0.5413, 0.4412,
         0.1595]])</code></pre>
</div>
</div>
<div id="97448ac8" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>attention_scores_for_small_keys <span class="op">=</span> projected_small_query <span class="op">@</span> projected_small_keys.T</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention scores for small keys:"</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_scores_for_small_keys)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>attention_scores_for_large_keys <span class="op">=</span> projected_large_query <span class="op">@</span> projected_large_keys.T</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention scores for large keys:"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_scores_for_large_keys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention scores for small keys:
tensor([0.5867, 0.7019, 0.3464, 0.4042])
attention scores for large keys:
tensor([11.8494, 16.6880,  7.7068,  7.4401])</code></pre>
</div>
</div>
<p>See how the attention score for a large projected key dimension is so much larger? Let’s see what happens if we just apply softmax to both the small and the large key:</p>
<div id="7dc39a2c" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>attention_weights_for_small_keys <span class="op">=</span> torch.softmax(attention_scores_for_small_keys, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention weights for small keys:"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_weights_for_small_keys)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>attention_weights_for_large_keys <span class="op">=</span> torch.softmax(attention_scores_for_large_keys, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"attention weights for large keys:"</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_weights_for_large_keys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention weights for small keys:
tensor([0.2673, 0.2999, 0.2102, 0.2227])
attention weights for large keys:
tensor([7.8542e-03, 9.9193e-01, 1.2473e-04, 9.5529e-05])</code></pre>
</div>
</div>
<p>If we now scale these values before applying softmax, we’ll get a different output that brings the result for a large projected key vector much closer in range compared to the small projected key:</p>
<div id="73343062" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>scaled_att_weights_for_small_keys <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  torch.softmax(attention_scores_for_small_keys <span class="op">/</span> small_proj_key_dim<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scaled attention weights for small keys:"</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scaled_att_weights_for_small_keys)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>scaled_att_weights_for_large_keys <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>  torch.softmax(attention_scores_for_large_keys <span class="op">/</span> large_proj_key_dim<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"scaled attention weights for large keys:"</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scaled_att_weights_for_large_keys)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>scaled attention weights for small keys:
tensor([0.2626, 0.2849, 0.2216, 0.2308])
scaled attention weights for large keys:
tensor([0.2498, 0.4574, 0.1488, 0.1440])</code></pre>
</div>
</div>
</section>
<section id="single-context-vector-2" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="single-context-vector-2"><span class="header-section-number">1.2.3</span> Single context vector (2)</h3>
<p>Now that we have our scaled attention weights, we can calculate the context vector for our query. We’ll do this by making a weighted combination of the projected values. Our scaled attention weights were:</p>
<div id="62d316f6" class="cell" data-execution_count="81">
<div class="cell-output cell-output-stdout">
<pre><code>scaled attention weights, one for each token:
tensor([0.2836, 0.2730, 0.2714, 0.1719])</code></pre>
</div>
</div>
<p>These we’ll multiply with our projected values. Our values were:</p>
<div id="960a94f8" class="cell" data-execution_count="82">
<div class="cell-output cell-output-stdout">
<pre><code>projected values:
tensor([[1.7842, 1.1135],
        [1.6554, 1.0571],
        [1.6442, 1.0264],
        [1.3340, 0.8176]])</code></pre>
</div>
</div>
<p>The calculated attention vector <span class="math inline">\(z^{(3)}\)</span> for query from <span class="math inline">\(x^{(3)}\)</span>:</p>
<div id="fac6f840" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>context_vector_x3 <span class="op">=</span> scaled_attention_weights_x3_as_query <span class="op">@</span> values</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"z3, the context vector for x3:"</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vector_x3)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"double check: the first number is the same as:"</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="fl">0.2773</span><span class="op">*</span><span class="fl">0.6307</span><span class="op">+</span><span class="fl">0.2594</span><span class="op">*</span><span class="fl">0.5699</span><span class="op">+</span><span class="fl">0.27</span><span class="op">*</span><span class="fl">0.8266</span><span class="op">+</span><span class="fl">0.1933</span><span class="op">*</span><span class="fl">0.6742</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"the second number is the same as:"</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="fl">0.2773</span><span class="op">*</span><span class="fl">0.4225</span><span class="op">+</span><span class="fl">0.2594</span><span class="op">*</span><span class="fl">0.3401</span><span class="op">+</span><span class="fl">0.27</span><span class="op">*</span><span class="fl">0.2332</span><span class="op">+</span><span class="fl">0.1933</span><span class="op">*</span><span class="fl">0.2259</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>z3, the context vector for x3:
tensor([1.6336, 1.0236])
double check: the first number is the same as:
0.67623003
the second number is the same as:
0.31201166</code></pre>
</div>
</div>
</section>
<section id="all-context-vectors-1" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="all-context-vectors-1"><span class="header-section-number">1.2.4</span> All context vectors</h3>
<p>Given everything we know now from above, we’ll create a torch module for self attention now:</p>
<div id="22b4bdfb" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention_1(nn.Module):</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out):</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> nn.Parameter(torch.rand(d_in, d_out))</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key <span class="op">=</span> nn.Parameter(torch.rand(d_in, d_out))</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> nn.Parameter(torch.rand(d_in, d_out))</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_key</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_query</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_value</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>        scaled_attention_weights <span class="op">=</span> <span class="op">\</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>            torch.softmax(</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>                attention_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>            , dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        context_vectors <span class="op">=</span> scaled_attention_weights <span class="op">@</span> values</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The way we can use this module using our same inputs as before:</p>
<div id="44c63556" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"inputs: "</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"d_in:"</span>, d_in, <span class="st">"d_out:"</span>, d_out)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>self_att <span class="op">=</span> SelfAttention_1(d_in, d_out)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>context_vectors_1 <span class="op">=</span> self_att(inputs)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"context vectors from SelfAttention v1:"</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vectors_1)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"for comparison, the context vector for x3:"</span>)</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vector_x3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inputs: 
tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936]])
d_in: 3 d_out: 2
context vectors from SelfAttention v1:
tensor([[1.6390, 1.0270],
        [1.6341, 1.0235],
        [1.6336, 1.0236],
        [1.6307, 1.0217]], grad_fn=&lt;MmBackward0&gt;)
for comparison, the context vector for x3:
tensor([1.6336, 1.0236])</code></pre>
</div>
</div>
<div id="69a15fc5" class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> plot_attention_heatmap</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention_2(nn.Module):</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in, d_out, kqv_bias<span class="op">=</span><span class="va">False</span>, plot_att_weights<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.plot_att_weights <span class="op">=</span> plot_att_weights</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_query <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>kqv_bias)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_key <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>kqv_bias)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_value <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>kqv_bias)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> <span class="va">self</span>.W_key(x)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> <span class="va">self</span>.W_query(x)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> <span class="va">self</span>.W_value(x)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>        scaled_attention_weights <span class="op">=</span> <span class="op">\</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>            torch.softmax(</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>                attention_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>            , dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.plot_att_weights:</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>            plot_attention_heatmap(scaled_attention_weights, [<span class="st">"I"</span>, <span class="st">"am"</span>, <span class="st">"learning"</span>, <span class="st">"this"</span>])</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        context_vectors <span class="op">=</span> scaled_attention_weights <span class="op">@</span> values</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1224fd81" class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"inputs: "</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"d_in:"</span>, d_in, <span class="st">"d_out:"</span>, d_out)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>self_att <span class="op">=</span> SelfAttention_2(d_in, d_out, kqv_bias<span class="op">=</span><span class="va">False</span>, plot_att_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>context_vectors_2 <span class="op">=</span> self_att(inputs)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"context vectors from SelfAttention v2:"</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vectors_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inputs: 
tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936]])
d_in: 3 d_out: 2</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-Attention_files/figure-html/cell-30-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>context vectors from SelfAttention v2:
tensor([[0.4536, 0.3638],
        [0.4547, 0.3629],
        [0.4642, 0.3538],
        [0.4657, 0.3523]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
<p>Note how the above result is different from our previous implementation, given a different strategy of initializing the weights for our linear layer.</p>
</section>
</section>
<section id="causal-self-attention" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="causal-self-attention"><span class="header-section-number">1.3</span> Causal self-attention</h2>
<p>At this point, we had every token attend to every other token. We need to introduce causality, making sure that when predicting the next token, the tokens before the current input cannot attend tokens that come after the current input. (After all, these still need to be predicted.) We’ll do this using what is called “masked attention”.</p>
<section id="naive-self-attention" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="naive-self-attention"><span class="header-section-number">1.3.1</span> Naive self-attention</h3>
<p>First we’ll set up the masking in a more naive way, not optimizing this yet just. Let’s first go back to calculating our queries, keys and attention scores. As a reminder: our inputs and <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_{key}\)</span> vectors:</p>
<div id="4e58ba0c" class="cell" data-execution_count="88">
<div class="cell-output cell-output-stdout">
<pre><code>inputs: tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936]])
inputs dimension: torch.Size([4, 3]) 

W_query weights: Parameter containing:
tensor([[ 0.4414,  0.4792, -0.1353],
        [ 0.5304, -0.1265,  0.1165]], requires_grad=True)
W_query dimension: torch.Size([2, 3]) 

W_key weights: Parameter containing:
tensor([[-0.2811,  0.3391,  0.5090],
        [-0.4236,  0.5018,  0.1081]], requires_grad=True)
W_key dimension: torch.Size([2, 3]) 
</code></pre>
</div>
</div>
<p>Let’s calculate our attention scores:</p>
<div id="d92d1fc9" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>queries <span class="op">=</span> self_att.W_query(inputs)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> self_att.W_key(inputs)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"queries: </span><span class="sc">{</span>queries<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"queries dimension: </span><span class="sc">{</span>queries<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"keys: </span><span class="sc">{</span>keys<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"keys dimension: </span><span class="sc">{</span>keys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>att_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"attention scores: </span><span class="sc">{</span>att_scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"attention scores dimension: </span><span class="sc">{</span>att_scores<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>att_weights <span class="op">=</span> torch.softmax(att_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"attention weights: </span><span class="sc">{</span>att_weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"attention weights dimension: </span><span class="sc">{</span>att_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>queries: tensor([[0.7761, 0.3968],
        [0.5293, 0.5294],
        [0.3663, 0.1453],
        [0.4264, 0.0216]], grad_fn=&lt;MmBackward0&gt;)
queries dimension: torch.Size([4, 2]) 

keys: tensor([[ 0.2571,  0.1269],
        [ 0.1686, -0.1455],
        [ 0.6758,  0.3913],
        [ 0.5816,  0.4767]], grad_fn=&lt;MmBackward0&gt;)
keys dimension: torch.Size([4, 2]) 

attention scores: tensor([[0.2499, 0.0731, 0.6797, 0.6405],
        [0.2032, 0.0122, 0.5648, 0.5602],
        [0.1126, 0.0406, 0.3044, 0.2823],
        [0.1124, 0.0687, 0.2966, 0.2582]], grad_fn=&lt;MmBackward0&gt;)
attention scores dimension: torch.Size([4, 4]) 

attention weights: tensor([[0.2195, 0.1937, 0.2975, 0.2893],
        [0.2246, 0.1962, 0.2900, 0.2891],
        [0.2368, 0.2250, 0.2712, 0.2670],
        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=&lt;SoftmaxBackward0&gt;)
attention weights dimension: torch.Size([4, 4]) 
</code></pre>
</div>
</div>
<p>We’ll now mask out the values beyond the current input token which should not be seen during training. We can do this by zeroing out everything above the diagonal of this matrix.</p>
<div id="d3bca65a" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> att_weights.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"context length: </span><span class="sc">{</span>context_length<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.tril(torch.ones((context_length, context_length)))</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"mask: </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>masked_att_weights <span class="op">=</span> att_weights <span class="op">*</span> mask</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked attention weights: </span><span class="sc">{</span>masked_att_weights<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>context length: 4 

mask: tensor([[1., 0., 0., 0.],
        [1., 1., 0., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 1.]]) 

masked attention weights: tensor([[0.2195, 0.0000, 0.0000, 0.0000],
        [0.2246, 0.1962, 0.0000, 0.0000],
        [0.2368, 0.2250, 0.2712, 0.0000],
        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=&lt;MulBackward0&gt;)</code></pre>
</div>
</div>
<p>Note how these values for each row don’t add up to one anymore.</p>
<div id="43f660fd" class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>rows_sum <span class="op">=</span> masked_att_weights.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"rows sum: </span><span class="sc">{</span>rows_sum<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>masked_att_weights_normalized <span class="op">=</span> masked_att_weights <span class="op">/</span> rows_sum</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked attention weights normalized: </span><span class="sc">{</span>masked_att_weights_normalized<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>rows sum: tensor([[0.2195],
        [0.4208],
        [0.7330],
        [1.0000]], grad_fn=&lt;SumBackward1&gt;) 

masked attention weights normalized: tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.5337, 0.4663, 0.0000, 0.0000],
        [0.3230, 0.3070, 0.3700, 0.0000],
        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=&lt;DivBackward0&gt;) 
</code></pre>
</div>
</div>
</section>
<section id="optimized-self-attention" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="optimized-self-attention"><span class="header-section-number">1.3.2</span> Optimized self-attention</h3>
<p>As a mathematical property for the softmax function, it treats <span class="math inline">\(-\infty\)</span> values as being 0 probabilities. As such we can turn our masking into:</p>
<div id="0e6c5d33" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.triu(torch.ones(context_length, context_length), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"mask: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>mask: 
 tensor([[0., 1., 1., 1.],
        [0., 0., 1., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 0.]]) 
</code></pre>
</div>
</div>
<div id="fa473342" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>masked_scores <span class="op">=</span> att_scores.masked_fill(mask.<span class="bu">bool</span>(), <span class="op">-</span> torch.inf)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked attention scores: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>masked_scores<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>masked_att_weights_normalized <span class="op">=</span> torch.softmax(masked_scores <span class="op">/</span> (keys.shape[<span class="op">-</span><span class="dv">1</span>]<span class="op">**-</span><span class="fl">0.5</span>), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked attention weights normalized: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>masked_att_weights_normalized<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>masked attention scores: 
 tensor([[0.2499,   -inf,   -inf,   -inf],
        [0.2032, 0.0122,   -inf,   -inf],
        [0.1126, 0.0406, 0.3044,   -inf],
        [0.1124, 0.0687, 0.2966, 0.2582]], grad_fn=&lt;MaskedFillBackward0&gt;) 

masked attention weights normalized: 
 tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.5671, 0.4329, 0.0000, 0.0000],
        [0.3111, 0.2810, 0.4080, 0.0000],
        [0.2239, 0.2105, 0.2905, 0.2752]], grad_fn=&lt;SoftmaxBackward0&gt;) 
</code></pre>
</div>
</div>
<p>From here we can compute the context vectors as before.</p>
<div id="8f38d813" class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>context_vectors_3 <span class="op">=</span> masked_att_weights_normalized <span class="op">@</span> self_att.W_value(inputs)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"context vectors  from self attention v3:"</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vectors_3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>context vectors v3:
tensor([[0.5545, 0.3680],
        [0.5772, 0.2723],
        [0.5198, 0.3241],
        [0.4590, 0.3586]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/xstof\.github\.io\/tinylm\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>