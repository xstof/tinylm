{
 "cells": [
  {
   "cell_type": "raw",
   "id": "96a0250e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "author: Christof Claessens\n",
    "date: June 2025\n",
    "title: \"Attention mechanics\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06802ab4",
   "metadata": {},
   "source": [
    "In the previous notebook we prepared and massaged our text into input vectors that encode both tokens as well as their positions.  It's these input vectors that will be handled by the rest of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3708c9",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "\n",
    "\n",
    "Self-Attention is a technique whereby each position in the input sequence can consider the relevancy of each other position in the same sequence when the representation for the sequence is being computed.  (Traditional \"attention\", as opposed to self-attention look at relations between two different sequences, input- and output, as opposed as using a single sequence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1690776",
   "metadata": {},
   "source": [
    "## Simplified Self-Attention\n",
    "\n",
    "Let's take a simple sentence:\n",
    "\"I am learning this\"\n",
    "\n",
    "This is our input sequencen let's call it $x$ with 4 tokens: $x^{(1)}$, $x^{(2)}$, $x^{(3)}$ and $x^{(4)}$.\n",
    "\n",
    "In general if our context length is $T$ then we have $x^{(1)} \\ldots x^{(T)}$\n",
    "\n",
    "Each $x^{(i)}$ is a $d$-dimensional embedding vector representing a token.\n",
    "\n",
    "Now we will calculate a context vector $z^{i}$ for each $x^{i}$.  This vector will contain information from all vectors $x^{(1)} \\ldots x^{(T)}$\n",
    "\n",
    "### Attention scores\n",
    "\n",
    "So, for example, for $x^{(3)}$ we'll calculate a context vector $z^{(3)}$.  We'll call $x^{(3)}$ our \"query\" vector and for this query we'll calculate an attention score for each of the tokens in our sentence: for example $w_{31}$ for the attention score between our query (the 3rd token) and the first token.  In this way we'll have: $w_{31}$, $w_{32}$, $w_{33}$ and $w_{34}$\n",
    "\n",
    "The attention score $w_{31}$ is the dot product from the third token with the first token.  So if:\n",
    "$$ x^{(3)} = [0.2,0.7,0.9] \\text{  (representing the token \"learning\")} $$\n",
    "$$ x^{(1)} = [0.8,0.9,0.3] \\text{  (representing the token \"I\")} $$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$w_{(31)} = x^{(3)} \\cdot x^{(1)} = [(.2*.8) + (.7*.9) + (.9 * .3)] = 1.06 $$\n",
    "\n",
    "We'll do this between our query $x^{(3)}$ and each of our $x^{(1)},x^{(2)},x^{(3)},x^{(4)}$ which result in an attention vector $w_3 = [ w_{(31)},w_{(32)},w_{(33)},w_{(34)} ]$\n",
    "\n",
    "Now in python, using an input vector of our 4 tokens, each with an embedding dimension of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87225baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n"
     ]
    }
   ],
   "source": [
    "#|echo: false\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embedding_dim = 3\n",
    "inputs = torch.rand((4,embedding_dim))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb69795",
   "metadata": {},
   "source": [
    "We'll calculate the attention vector for $x^{(3)}$ as our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df259370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3127, 1.1213, 1.5807, 1.3343])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "attention_score_for_x3 = torch.empty((4))\n",
    "for i, xi in enumerate(inputs):\n",
    "    # Calculate the attention score for x3i\n",
    "    attention_score_for_x3[i] = torch.dot(xi, query)\n",
    "\n",
    "print(attention_score_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8fb5d",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "Now that we've calculated the attention vector $w_3$ for query $x^{(3)}$, we have a vector with a number for every token.  These numbers however are not normalized between $0$ and $1$, which is what we'd really like.  For this, we'll use [Cross-Entropy Loss](../theory/cross-entropy-loss.ipynb), which will make sure the numbers in our attention vector add up to one and each are between $0 \\ldots 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26d7f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2407, 0.1987, 0.3146, 0.2459])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_for_x3 = torch.softmax(attention_score_for_x3, dim=0)\n",
    "print(attention_weights_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9608f2",
   "metadata": {},
   "source": [
    "### Context Vector\n",
    "\n",
    "So now that we have our normalized attention weights for a single query $x^{(3)}$ we can calculate the full context vector that corresponds to $x^{(3)}$\n",
    "\n",
    "Our attention weights from the previous step were: $[.2, .1, .3, .2]$ or more in general, a vector $[\\alpha _{31}, \\alpha _{32}, \\alpha _{33}, \\alpha _{34}]$\n",
    "\n",
    "To calculate the context vector for $x^{(3)}$ we'll take each $\\alpha _{3i}$ and multiply that by $x^{(i)}$.  Then we'll add up all those vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66414db4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68002bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5165, 0.7774, 0.6536])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "context_vector_for_x3 = torch.zeros((query.shape)) # dimension of query (3 in this case)\n",
    "\n",
    "for i, xi in enumerate(inputs):\n",
    "    context_vector_for_x3 += attention_weights_for_x3[i] * xi # z3\n",
    "\n",
    "print(context_vector_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402b7ce",
   "metadata": {},
   "source": [
    "### All context vectors\n",
    "\n",
    "What we did so far is to look at how to calculate a single context vector $z^{(3)}$, for a single token $x^{(3)}$ in our input sequence.  We'll need to make this more scalable and figure out a way to calculate $z$ for all tokens in our input sequence.\n",
    "\n",
    "This means, what we have done for $x^{(3)}$ we need to do for all inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "028b9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores:\n",
      "tensor([[1.7622, 1.4337, 1.3127, 1.1999],\n",
      "        [1.4337, 1.4338, 1.1213, 0.8494],\n",
      "        [1.3127, 1.1213, 1.5807, 1.3343],\n",
      "        [1.1999, 0.8494, 1.3343, 1.2435]])\n",
      "for comparison, the attention scores for x3 as query:\n",
      "tensor([1.3127, 1.1213, 1.5807, 1.3343])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_manual = torch.empty((4, 4))  # for each query of the 4 inputs, calculate 4 attention scores\n",
    "for i, query in enumerate(inputs):\n",
    "    for j, xi in enumerate(inputs):\n",
    "        attention_scores_manual[i][j] = torch.dot(query, xi)\n",
    "\n",
    "print(\"attention scores:\")\n",
    "print(attention_scores_manual)\n",
    "print(\"for comparison, the attention scores for x3 as query:\")\n",
    "print(attention_score_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdfed7",
   "metadata": {},
   "source": [
    "Using `for` loops however is slow and can't be optimized using CUDA, so let's find a way to do the same, but to use pure tensor calculations instead.  Our inputs look like the below, with 4 tokens, each with a dimension of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4de671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55213d81",
   "metadata": {},
   "source": [
    "We can transpose this vector, so it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ff2e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9593, 0.2566, 0.1332],\n",
      "        [0.9150, 0.3904, 0.7936, 0.9346],\n",
      "        [0.3829, 0.6009, 0.9408, 0.5936]])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(inputs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47699e09",
   "metadata": {},
   "source": [
    "If we now take multiply these two matrices, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbb777d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7622, 1.4337, 1.3127, 1.1999],\n",
      "        [1.4337, 1.4338, 1.1213, 0.8494],\n",
      "        [1.3127, 1.1213, 1.5807, 1.3343],\n",
      "        [1.1999, 0.8494, 1.3343, 1.2435]])\n",
      "for comparison, the attention scores for x3 as query:\n",
      "tensor([1.3127, 1.1213, 1.5807, 1.3343])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T  # matrix multiplication\n",
    "print(inputs @ inputs.T) # matrix multiplication\n",
    "print(\"for comparison, the attention scores for x3 as query:\")\n",
    "print(attention_score_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd878a",
   "metadata": {},
   "source": [
    "These are just attention scores, not yet attention weights, so lets normalize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ac7edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights:\n",
      "tensor([[0.3415, 0.2459, 0.2179, 0.1946],\n",
      "        [0.3040, 0.3040, 0.2225, 0.1695],\n",
      "        [0.2407, 0.1987, 0.3146, 0.2459],\n",
      "        [0.2569, 0.1809, 0.2938, 0.2683]])\n",
      "for comparison, the attention weights for x3 as query:\n",
      "tensor([0.2407, 0.1987, 0.3146, 0.2459])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)  # normalize the attention scores\n",
    "print(\"attention weights:\")\n",
    "print(attention_weights)\n",
    "print(\"for comparison, the attention weights for x3 as query:\")\n",
    "print(attention_weights_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6c169",
   "metadata": {},
   "source": [
    "From here we can calculate our context vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba50a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vectors for all inputs:\n",
      "tensor([[0.6191, 0.7634, 0.5991],\n",
      "        [0.6395, 0.7318, 0.6090],\n",
      "        [0.5165, 0.7774, 0.6536],\n",
      "        [0.5113, 0.7897, 0.6428]])\n",
      "for comparison, the context vector for x3:\n",
      "tensor([0.5165, 0.7774, 0.6536])\n"
     ]
    }
   ],
   "source": [
    "all_context_vectors = attention_weights @ inputs  # matrix multiplication\n",
    "print(\"context vectors for all inputs:\")\n",
    "print(all_context_vectors)\n",
    "print(\"for comparison, the context vector for x3:\")\n",
    "print(context_vector_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153439",
   "metadata": {},
   "source": [
    "## Self-Attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f3b84",
   "metadata": {},
   "source": [
    "From here we'll expand to what is called \"scaled dot-product attention\".  Also here we'll want to calculate context vectors (one for each of our input tokens) as a weighted sum over (some abstraction) of the inputs.  There are some differences with what we've done so far though:\n",
    "\n",
    "- instead of taking directly $q^{(i)} = x^{(i)}$ as the query vector, we'll use a projection of $x^{(i)}$.  We do this using a trainable weight matrix $W_q$, used to calculate our query\n",
    "- instead of taking directly the dot product of $q^{(i)}$ and each $x^{(i)}$ to calculate our attention scores and weights, we'll do the dot product with a projection of $x^{(i)}$.  We do this using a trainable weight matrix $W_k$, used to calculate our keys.\n",
    "- instead of calculating the weighted average using these scores with each $x^{(i)}$, we'll do this with a projection of $x^{(i)}$.  We do this using a trainable weight matrix $W_v$, used to calculate our values.\n",
    "\n",
    "We'll have trainable weight matrixes: $W_q$, $W_k$, $W_v$ so that the model, when trained can learn to use these to project respectively the query, key and value vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c18386",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79b46f0",
   "metadata": {},
   "source": [
    "### Single context vector (1)\n",
    "\n",
    "Our token embedding vectors are of a certain dimension `embedding_dim = 3` in our example.  We can choose to project these into another dimension, from 3 to 5 for example.  (This is not usually done however, it's likely kept the same.)  For illustration purposes, let's go with 5 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2db615f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "d_in = embedding_dim\n",
    "d_out = 2\n",
    "\n",
    "W_query_1 = torch.rand((d_in, d_out))\n",
    "W_key_1 = torch.rand((d_in, d_out))\n",
    "W_value_1 = torch.rand((d_in, d_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731ad4a",
   "metadata": {},
   "source": [
    "We can now use  $W_q$, $W_k$, $W_v$ to project an embedding vector from it's normal dimension into a dimension of 5.  Let's use $x^{(3)}$ as an example for our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6980ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 input:\n",
      "tensor([0.2566, 0.7936, 0.9408])\n",
      "W_query:\n",
      "tensor([[0.8823, 0.9150],\n",
      "        [0.3829, 0.9593],\n",
      "        [0.3904, 0.6009]])\n",
      "projected query:\n",
      "tensor([0.8975, 1.5614])\n"
     ]
    }
   ],
   "source": [
    "x3 = inputs[2]\n",
    "print(\"x3 input:\")\n",
    "print(x3)\n",
    "\n",
    "print(\"W_query:\")\n",
    "print(W_query_1)\n",
    "\n",
    "query = x3 @ W_query_1\n",
    "print(\"projected query:\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde87030",
   "metadata": {},
   "source": [
    "Instead of defining our projection matrices like above, we'll define them as pytorch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5938b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = torch.nn.Parameter(W_query_1, requires_grad=False)\n",
    "W_key = torch.nn.Parameter(W_key_1, requires_grad=False)\n",
    "W_value = torch.nn.Parameter(W_value_1, requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d67d5",
   "metadata": {},
   "source": [
    "Let's calculate our key and value vectors for every input token in our sentence \"I am learning this\": $x^{(1)}$: \"I\", $x^{(2)}$: \"am\", $x^{(3)}$: \"learning\", $x^{(4)}$: \"this\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b833c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      "tensor([[1.7842, 1.1135],\n",
      "        [1.6554, 1.0571],\n",
      "        [1.6442, 1.0264],\n",
      "        [1.3340, 0.8176]])\n",
      "keys:\n",
      "tensor([[1.4450, 1.0493],\n",
      "        [1.1750, 1.1700],\n",
      "        [1.6917, 0.8678],\n",
      "        [1.4682, 0.5825]])\n"
     ]
    }
   ],
   "source": [
    "values = inputs @ W_value\n",
    "print(\"values:\")\n",
    "print(values)\n",
    "keys = inputs @ W_key\n",
    "print(\"keys:\")\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1fc7f",
   "metadata": {},
   "source": [
    "Now we can calculate the attention scores, for our query vector that got projected from $x^{(3)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55f57929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query projection from x3:\n",
      "tensor([0.8975, 1.5614])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(\"query projection from x3:\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa849c5",
   "metadata": {},
   "source": [
    "The attention score $w_{31}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51c04b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 input:\n",
      "tensor([0.8823, 0.9150, 0.3829])\n",
      "key projection from x1: \n",
      "tensor([1.4450, 1.0493])\n",
      "... which is the same as: \n",
      "tensor([1.4450, 1.0493])\n",
      "attention score between our query and x1's projected key:\n",
      "tensor(2.9354)\n"
     ]
    }
   ],
   "source": [
    "x1 = inputs[0]\n",
    "print(\"x1 input:\")\n",
    "print(x1)\n",
    "print(\"key projection from x1: \")\n",
    "key = x1 @ W_key\n",
    "print(key)\n",
    "print(\"... which is the same as: \")\n",
    "print(keys[0])  # keys[0] is the key for x1\n",
    "\n",
    "print(\"attention score between our query and x1's projected key:\")\n",
    "attention_score_x3_x1 = query.dot(key)\n",
    "print(attention_score_x3_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de57453",
   "metadata": {},
   "source": [
    "In the same way as we got the attention score between the query and x1, we can get all attention scores for our query like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8599d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores for x3 as query:\n",
      "tensor([2.9354, 2.8816, 2.8733, 2.2273])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_x3_as_query = query @ keys.T\n",
    "print(\"attention scores for x3 as query:\")\n",
    "print(attention_scores_x3_as_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead7b06",
   "metadata": {},
   "source": [
    "As is shown above, for our chosen query, we'll end up with 4 attention scores: one for each input token.  Like before, we'll want to normalize our attention scores into attention weights but instead of doing a pure softmax function, we'll first scale the attention scores by dividing them by the square root of the dimension of our projected keys (2 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ee0dbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of our projected key:  2\n",
      "scaled attention weights for x3 as query:\n",
      "tensor([0.2836, 0.2730, 0.2714, 0.1719])\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension of our projected key: \", d_out)\n",
    "scaled_attention_weights_x3_as_query = \\\n",
    "  torch.softmax(attention_scores_x3_as_query / d_out**0.5, dim=-1)\n",
    "print(\"scaled attention weights for x3 as query:\")\n",
    "print(scaled_attention_weights_x3_as_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195a0c3",
   "metadata": {},
   "source": [
    "We don't have our full context vector yet.  For that we'll still need to make a weighted combination of our projected value vectors.  Before we do so, let's digress a little on the need for scaling the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe247b",
   "metadata": {},
   "source": [
    "### Why scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fab30",
   "metadata": {},
   "source": [
    "As the dimension for our projected key vector grows, the dot products can become large numbers.  An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d45e5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of the key:  3\n",
      "test token embeddings for 4 token:\n",
      "tensor([[0.2666, 0.6274, 0.2696],\n",
      "        [0.4414, 0.2969, 0.8317],\n",
      "        [0.1053, 0.2695, 0.3588],\n",
      "        [0.1994, 0.5472, 0.0062]])\n",
      "test query:\n",
      "tensor([0.1053, 0.2695, 0.3588])\n",
      "projected small query:\n",
      "tensor([0.4380, 0.3182])\n",
      "projected small keys:\n",
      "tensor([[0.9006, 0.6041],\n",
      "        [0.9639, 0.8792],\n",
      "        [0.4601, 0.4554],\n",
      "        [0.6766, 0.3391]])\n",
      "projected large query:\n",
      "tensor([0.1878, 0.3446, 0.4501, 0.3379, 0.3108, 0.1170, 0.1112, 0.4571, 0.4659,\n",
      "        0.4757, 0.2060, 0.3174, 0.3258, 0.3598, 0.2669, 0.5505, 0.6264, 0.1989,\n",
      "        0.5678, 0.3476, 0.3972, 0.4965, 0.3051, 0.4656, 0.3402, 0.3913, 0.4238,\n",
      "        0.1793, 0.2759, 0.1342, 0.4060, 0.4709, 0.5078, 0.2881, 0.1066, 0.2342,\n",
      "        0.5123, 0.1315, 0.3327, 0.5920, 0.3685, 0.3478, 0.2677, 0.1843, 0.1589,\n",
      "        0.1085, 0.1547, 0.3190, 0.4401, 0.2056, 0.4259, 0.4452, 0.4818, 0.3970,\n",
      "        0.4401, 0.4995, 0.1077, 0.2400, 0.3922, 0.2375, 0.1725, 0.5728, 0.4268,\n",
      "        0.3151])\n",
      "projected large keys:\n",
      "tensor([[0.3035, 0.5216, 0.4596, 0.2893, 0.8955, 0.6188, 0.6455, 0.2561, 0.3482,\n",
      "         0.2990, 0.5600, 0.4848, 0.5095, 0.4012, 0.6952, 0.5336, 0.7148, 0.2508,\n",
      "         0.3392, 0.3222, 0.4917, 0.2481, 0.9719, 0.6077, 0.9490, 0.5704, 0.2386,\n",
      "         0.7114, 0.6478, 0.5438, 0.3272, 0.1102, 0.6529, 0.4615, 0.6076, 0.2971,\n",
      "         0.7800, 0.3415, 0.4149, 0.6020, 0.9005, 0.8435, 0.8293, 0.2204, 0.7158,\n",
      "         0.4737, 0.4997, 0.3527, 0.9515, 0.2662, 0.7417, 0.8811, 0.5258, 0.3140,\n",
      "         0.8549, 0.6960, 0.6771, 0.2255, 0.5432, 0.4350, 0.3110, 0.8551, 0.7196,\n",
      "         0.4307],\n",
      "        [0.5414, 0.8804, 1.0148, 0.4877, 0.8235, 0.7101, 0.6321, 0.6603, 0.7909,\n",
      "         0.2588, 0.7355, 1.0503, 0.8600, 0.6394, 1.0722, 0.9444, 1.1029, 0.4545,\n",
      "         0.5869, 0.5922, 0.7288, 0.4679, 1.2429, 1.0669, 1.0546, 0.6073, 0.5337,\n",
      "         0.7313, 1.0426, 0.9182, 0.4964, 0.3018, 0.9588, 0.7419, 0.6737, 0.5018,\n",
      "         0.7198, 0.4869, 0.5098, 0.8060, 1.2439, 1.0076, 0.6866, 0.3648, 0.8830,\n",
      "         0.9519, 0.7175, 0.6335, 1.1389, 0.4614, 0.7267, 1.2907, 0.4841, 0.5035,\n",
      "         0.7083, 1.0215, 1.0409, 0.5922, 0.4310, 0.4074, 0.3501, 1.0624, 1.0229,\n",
      "         0.9020],\n",
      "        [0.1838, 0.3311, 0.3795, 0.2121, 0.4295, 0.3402, 0.3010, 0.2730, 0.3651,\n",
      "         0.1327, 0.3892, 0.3841, 0.3565, 0.2441, 0.4775, 0.4100, 0.4658, 0.2066,\n",
      "         0.2301, 0.2715, 0.3052, 0.1889, 0.6106, 0.4833, 0.5087, 0.3374, 0.2239,\n",
      "         0.4179, 0.4202, 0.3830, 0.1427, 0.1232, 0.5042, 0.3661, 0.3749, 0.1694,\n",
      "         0.3759, 0.1509, 0.2579, 0.4196, 0.5787, 0.5371, 0.3629, 0.1396, 0.4025,\n",
      "         0.4415, 0.3514, 0.2355, 0.5749, 0.1862, 0.3541, 0.5809, 0.2908, 0.2299,\n",
      "         0.3590, 0.4987, 0.5014, 0.2625, 0.2681, 0.1827, 0.1524, 0.5781, 0.4842,\n",
      "         0.4073],\n",
      "        [0.1846, 0.3146, 0.1962, 0.1578, 0.7056, 0.4448, 0.5090, 0.0615, 0.1004,\n",
      "         0.2439, 0.3434, 0.2190, 0.2877, 0.2528, 0.4102, 0.2744, 0.4366, 0.1196,\n",
      "         0.1940, 0.1490, 0.3129, 0.1252, 0.6383, 0.3039, 0.6941, 0.4018, 0.0853,\n",
      "         0.5061, 0.3915, 0.3056, 0.2424, 0.0210, 0.3584, 0.2398, 0.4157, 0.1903,\n",
      "         0.6133, 0.2556, 0.2766, 0.3666, 0.5725, 0.5584, 0.6834, 0.1339, 0.5017,\n",
      "         0.1842, 0.2965, 0.2012, 0.6479, 0.1490, 0.5800, 0.5425, 0.3916, 0.1746,\n",
      "         0.7133, 0.4049, 0.3785, 0.0399, 0.4320, 0.3572, 0.2354, 0.5413, 0.4412,\n",
      "         0.1595]])\n"
     ]
    }
   ],
   "source": [
    "key_dim = 3\n",
    "print(\"dimension of the key: \", key_dim)\n",
    "small_proj_key_dim = 2\n",
    "large_proj_key_dim = 64\n",
    "\n",
    "test_token_embeddings = torch.rand((4, key_dim)) # 4 tokens, each with key_dim features\n",
    "print(\"test token embeddings for 4 token:\")\n",
    "print(test_token_embeddings)\n",
    "\n",
    "test_query  = test_token_embeddings[2]  # let's take the 3rd token as query\n",
    "print(\"test query:\")\n",
    "print(test_query)\n",
    "\n",
    "W_k_for_small_output_dim = torch.rand(key_dim, small_proj_key_dim)\n",
    "W_q_for_small_output_dim = torch.rand(key_dim, small_proj_key_dim)\n",
    "W_k_for_large_output_dim = torch.rand(key_dim, large_proj_key_dim)\n",
    "W_q_for_large_output_dim = torch.rand(key_dim, large_proj_key_dim)\n",
    "\n",
    "projected_small_keys = test_token_embeddings @ W_k_for_small_output_dim\n",
    "projected_small_query = test_query @ W_q_for_small_output_dim\n",
    "projected_large_keys = test_token_embeddings @ W_k_for_large_output_dim\n",
    "projected_large_query = test_query @ W_q_for_large_output_dim\n",
    "\n",
    "print(\"projected small query:\")\n",
    "print(projected_small_query)\n",
    "print(\"projected small keys:\")\n",
    "print(projected_small_keys)\n",
    "print(\"projected large query:\")\n",
    "print(projected_large_query)\n",
    "print(\"projected large keys:\")\n",
    "print(projected_large_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97448ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores for small keys:\n",
      "tensor([0.5867, 0.7019, 0.3464, 0.4042])\n",
      "attention scores for large keys:\n",
      "tensor([11.8494, 16.6880,  7.7068,  7.4401])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_for_small_keys = projected_small_query @ projected_small_keys.T\n",
    "print(\"attention scores for small keys:\")\n",
    "print(attention_scores_for_small_keys)\n",
    "attention_scores_for_large_keys = projected_large_query @ projected_large_keys.T\n",
    "print(\"attention scores for large keys:\")\n",
    "print(attention_scores_for_large_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5fede",
   "metadata": {},
   "source": [
    "See how the attention score for a large projected key dimension is so much larger?  Let's see what happens if we just apply softmax to both the small and the large key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dc39a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights for small keys:\n",
      "tensor([0.2673, 0.2999, 0.2102, 0.2227])\n",
      "attention weights for large keys:\n",
      "tensor([7.8542e-03, 9.9193e-01, 1.2473e-04, 9.5529e-05])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_for_small_keys = torch.softmax(attention_scores_for_small_keys, dim=0)\n",
    "print(\"attention weights for small keys:\")\n",
    "print(attention_weights_for_small_keys)\n",
    "attention_weights_for_large_keys = torch.softmax(attention_scores_for_large_keys, dim=0)\n",
    "print(\"attention weights for large keys:\")\n",
    "print(attention_weights_for_large_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38acd5e",
   "metadata": {},
   "source": [
    "If we now scale these values before applying softmax, we'll get a different output that brings the result for a large projected key vector much closer in range compared to the small projected key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73343062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled attention weights for small keys:\n",
      "tensor([0.2626, 0.2849, 0.2216, 0.2308])\n",
      "scaled attention weights for large keys:\n",
      "tensor([0.2498, 0.4574, 0.1488, 0.1440])\n"
     ]
    }
   ],
   "source": [
    "scaled_att_weights_for_small_keys = \\\n",
    "  torch.softmax(attention_scores_for_small_keys / small_proj_key_dim**0.5, dim=0)\n",
    "print(\"scaled attention weights for small keys:\")\n",
    "print(scaled_att_weights_for_small_keys)\n",
    "scaled_att_weights_for_large_keys = \\\n",
    "  torch.softmax(attention_scores_for_large_keys / large_proj_key_dim**0.5, dim=0)\n",
    "print(\"scaled attention weights for large keys:\")\n",
    "print(scaled_att_weights_for_large_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ad3b2",
   "metadata": {},
   "source": [
    "### Single context vector (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c438b",
   "metadata": {},
   "source": [
    "Now that we have our scaled attention weights, we can calculate the context vector for our query.  We'll do this by making a weighted combination of the projected values.  Our scaled attention weights were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62d316f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled attention weights, one for each token:\n",
      "tensor([0.2836, 0.2730, 0.2714, 0.1719])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(\"scaled attention weights, one for each token:\")\n",
    "print(scaled_attention_weights_x3_as_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3b26c",
   "metadata": {},
   "source": [
    "These we'll multiply with our projected values.  Our values were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "960a94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected values:\n",
      "tensor([[1.7842, 1.1135],\n",
      "        [1.6554, 1.0571],\n",
      "        [1.6442, 1.0264],\n",
      "        [1.3340, 0.8176]])\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(\"projected values:\")\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c14eb0",
   "metadata": {},
   "source": [
    "The calculated attention vector $z^{(3)}$ for query from $x^{(3)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fac6f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z3, the context vector for x3:\n",
      "tensor([1.6336, 1.0236])\n",
      "double check: the first number is the same as:\n",
      "0.67623003\n",
      "the second number is the same as:\n",
      "0.31201166\n"
     ]
    }
   ],
   "source": [
    "context_vector_x3 = scaled_attention_weights_x3_as_query @ values\n",
    "print(\"z3, the context vector for x3:\")\n",
    "print(context_vector_x3)\n",
    "print(\"double check: the first number is the same as:\")\n",
    "print(0.2773*0.6307+0.2594*0.5699+0.27*0.8266+0.1933*0.6742)\n",
    "print(\"the second number is the same as:\")\n",
    "print(0.2773*0.4225+0.2594*0.3401+0.27*0.2332+0.1933*0.2259)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123add8",
   "metadata": {},
   "source": [
    "### All context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61251ad7",
   "metadata": {},
   "source": [
    "Given everything we know now from above, we'll create a torch module for self attention now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22b4bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "class SelfAttention_1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        scaled_attention_weights = \\\n",
    "            torch.softmax(\n",
    "                attention_scores / keys.shape[-1]**0.5\n",
    "            , dim=-1)\n",
    "        context_vectors = scaled_attention_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b037a4d",
   "metadata": {},
   "source": [
    "The way we can use this module using our same inputs as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44c63556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "d_in: 3 d_out: 2\n",
      "context vectors from SelfAttention v1:\n",
      "tensor([[1.6390, 1.0270],\n",
      "        [1.6341, 1.0235],\n",
      "        [1.6336, 1.0236],\n",
      "        [1.6307, 1.0217]], grad_fn=<MmBackward0>)\n",
      "for comparison, the context vector for x3:\n",
      "tensor([1.6336, 1.0236])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "print(\"inputs: \")\n",
    "print(inputs)\n",
    "print(\"d_in:\", d_in, \"d_out:\", d_out)\n",
    "\n",
    "self_att = SelfAttention_1(d_in, d_out)\n",
    "context_vectors_1 = self_att(inputs)\n",
    "print(\"context vectors from SelfAttention v1:\")\n",
    "print(context_vectors_1)\n",
    "print(\"for comparison, the context vector for x3:\")\n",
    "print(context_vector_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69a15fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # for reproducibility\n",
    "from utils import plot_attention_heatmap\n",
    "\n",
    "class SelfAttention_2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, kqv_bias=False, plot_att_weights=False):\n",
    "        super().__init__()\n",
    "        self.plot_att_weights = plot_att_weights\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        scaled_attention_weights = \\\n",
    "            torch.softmax(\n",
    "                attention_scores / keys.shape[-1]**0.5\n",
    "            , dim=-1)\n",
    "        \n",
    "        if self.plot_att_weights:\n",
    "            plot_attention_heatmap(scaled_attention_weights, [\"I\", \"am\", \"learning\", \"this\"])\n",
    "        \n",
    "        context_vectors = scaled_attention_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1224fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "d_in: 3 d_out: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vectors from SelfAttention v2:\n",
      "tensor([[0.4536, 0.3638],\n",
      "        [0.4547, 0.3629],\n",
      "        [0.4642, 0.3538],\n",
      "        [0.4657, 0.3523]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "print(\"inputs: \")\n",
    "print(inputs)\n",
    "print(\"d_in:\", d_in, \"d_out:\", d_out)\n",
    "\n",
    "self_att = SelfAttention_2(d_in, d_out, kqv_bias=False, plot_att_weights=True)\n",
    "context_vectors_2 = self_att(inputs)\n",
    "print(\"context vectors from SelfAttention v2:\")\n",
    "print(context_vectors_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338cd86",
   "metadata": {},
   "source": [
    "Note how the above result is different from our previous implementation, given a different strategy of initializing the weights for our linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67602f",
   "metadata": {},
   "source": [
    "## Causal self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa973ec1",
   "metadata": {},
   "source": [
    "At this point, we had every token attend to every other token.  We need to introduce causality, making sure that when predicting the next token, the tokens before the current input cannot attend tokens that come after the current input.  (After all, these still need to be predicted.)  We'll do this using what is called \"masked attention\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd946ef",
   "metadata": {},
   "source": [
    "### Naive self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b26d8",
   "metadata": {},
   "source": [
    "First we'll set up the masking in a more naive way, not optimizing this yet just.  Let's first go back to calculating our queries, keys and attention scores.  As a reminder: our inputs and $W_q$, $W_{key}$ vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e58ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "inputs dimension: torch.Size([4, 3]) \n",
      "\n",
      "W_query weights: Parameter containing:\n",
      "tensor([[ 0.4414,  0.4792, -0.1353],\n",
      "        [ 0.5304, -0.1265,  0.1165]], requires_grad=True)\n",
      "W_query dimension: torch.Size([2, 3]) \n",
      "\n",
      "W_key weights: Parameter containing:\n",
      "tensor([[-0.2811,  0.3391,  0.5090],\n",
      "        [-0.4236,  0.5018,  0.1081]], requires_grad=True)\n",
      "W_key dimension: torch.Size([2, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(f\"inputs: {inputs}\")\n",
    "print(f\"inputs dimension: {inputs.shape} \\n\")\n",
    "print(f\"W_query weights: {self_att.W_query.weight}\")\n",
    "print(f\"W_query dimension: {self_att.W_query.weight.shape} \\n\")\n",
    "print(f\"W_key weights: {self_att.W_key.weight}\")\n",
    "print(f\"W_key dimension: {self_att.W_key.weight.shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8fc4b",
   "metadata": {},
   "source": [
    "Let's calculate our attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d92d1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: tensor([[0.7761, 0.3968],\n",
      "        [0.5293, 0.5294],\n",
      "        [0.3663, 0.1453],\n",
      "        [0.4264, 0.0216]], grad_fn=<MmBackward0>)\n",
      "queries dimension: torch.Size([4, 2]) \n",
      "\n",
      "keys: tensor([[ 0.2571,  0.1269],\n",
      "        [ 0.1686, -0.1455],\n",
      "        [ 0.6758,  0.3913],\n",
      "        [ 0.5816,  0.4767]], grad_fn=<MmBackward0>)\n",
      "keys dimension: torch.Size([4, 2]) \n",
      "\n",
      "attention scores: tensor([[0.2499, 0.0731, 0.6797, 0.6405],\n",
      "        [0.2032, 0.0122, 0.5648, 0.5602],\n",
      "        [0.1126, 0.0406, 0.3044, 0.2823],\n",
      "        [0.1124, 0.0687, 0.2966, 0.2582]], grad_fn=<MmBackward0>)\n",
      "attention scores dimension: torch.Size([4, 4]) \n",
      "\n",
      "attention weights: tensor([[0.2195, 0.1937, 0.2975, 0.2893],\n",
      "        [0.2246, 0.1962, 0.2900, 0.2891],\n",
      "        [0.2368, 0.2250, 0.2712, 0.2670],\n",
      "        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=<SoftmaxBackward0>)\n",
      "attention weights dimension: torch.Size([4, 4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = self_att.W_query(inputs)\n",
    "keys = self_att.W_key(inputs)\n",
    "\n",
    "print(f\"queries: {queries}\")\n",
    "print(f\"queries dimension: {queries.shape} \\n\")\n",
    "print(f\"keys: {keys}\")\n",
    "print(f\"keys dimension: {keys.shape} \\n\")\n",
    "att_scores = queries @ keys.T\n",
    "print(f\"attention scores: {att_scores}\")\n",
    "print(f\"attention scores dimension: {att_scores.shape} \\n\")\n",
    "att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(f\"attention weights: {att_weights}\")\n",
    "print(f\"attention weights dimension: {att_weights.shape} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625dec5",
   "metadata": {},
   "source": [
    "We'll now mask out the values beyond the current input token which should not be seen during training.  We can do this by zeroing out everything above the diagonal of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3bca65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context length: 4 \n",
      "\n",
      "mask: tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "masked attention weights: tensor([[0.2195, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2246, 0.1962, 0.0000, 0.0000],\n",
      "        [0.2368, 0.2250, 0.2712, 0.0000],\n",
      "        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = att_weights.shape[-1]\n",
    "print(f\"context length: {context_length} \\n\")\n",
    "\n",
    "mask = torch.tril(torch.ones((context_length, context_length)))\n",
    "print(f\"mask: {mask} \\n\")\n",
    "\n",
    "masked_att_weights = att_weights * mask\n",
    "print(f\"masked attention weights: {masked_att_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584dba0",
   "metadata": {},
   "source": [
    "Note how these values for each row don't add up to one anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43f660fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows sum: tensor([[0.2195],\n",
      "        [0.4208],\n",
      "        [0.7330],\n",
      "        [1.0000]], grad_fn=<SumBackward1>) \n",
      "\n",
      "masked attention weights normalized: tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5337, 0.4663, 0.0000, 0.0000],\n",
      "        [0.3230, 0.3070, 0.3700, 0.0000],\n",
      "        [0.2371, 0.2299, 0.2701, 0.2629]], grad_fn=<DivBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_sum = masked_att_weights.sum(dim=-1, keepdim=True)\n",
    "print(f\"rows sum: {rows_sum} \\n\")\n",
    "masked_att_weights_normalized = masked_att_weights / rows_sum\n",
    "print(f\"masked attention weights normalized: {masked_att_weights_normalized} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9cd55",
   "metadata": {},
   "source": [
    "### Optimized self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977a82c",
   "metadata": {},
   "source": [
    "As a mathematical property for the softmax function, it treats $-\\infty$ values as being 0 probabilities.  As such we can turn our masking into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0e6c5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: \n",
      " tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(f\"mask: \\n {mask} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa473342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked attention scores: \n",
      " tensor([[0.2499,   -inf,   -inf,   -inf],\n",
      "        [0.2032, 0.0122,   -inf,   -inf],\n",
      "        [0.1126, 0.0406, 0.3044,   -inf],\n",
      "        [0.1124, 0.0687, 0.2966, 0.2582]], grad_fn=<MaskedFillBackward0>) \n",
      "\n",
      "masked attention weights normalized: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5671, 0.4329, 0.0000, 0.0000],\n",
      "        [0.3111, 0.2810, 0.4080, 0.0000],\n",
      "        [0.2239, 0.2105, 0.2905, 0.2752]], grad_fn=<SoftmaxBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked_scores = att_scores.masked_fill(mask.bool(), - torch.inf)\n",
    "print(f\"masked attention scores: \\n {masked_scores} \\n\")\n",
    "masked_att_weights_normalized = torch.softmax(masked_scores / (keys.shape[-1]**-0.5), dim=1)\n",
    "print(f\"masked attention weights normalized: \\n {masked_att_weights_normalized} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16142cab",
   "metadata": {},
   "source": [
    "From here we can compute the context vectors as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vectors v3:\n",
      "tensor([[0.5545, 0.3680],\n",
      "        [0.5772, 0.2723],\n",
      "        [0.5198, 0.3241],\n",
      "        [0.4590, 0.3586]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_vectors_3 = masked_att_weights_normalized @ self_att.W_value(inputs)\n",
    "print(\"context vectors  from self attention v3:\")\n",
    "print(context_vectors_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf32bf9",
   "metadata": {},
   "source": [
    "### Dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb11d59",
   "metadata": {},
   "source": [
    "Dropout is a technique where at random some units are being ignored during training (and only during training).  It prevents overfitting.  In transformers we'll either do this:\n",
    "- (most common) after calculating the attention weights, or\n",
    "- after the attention weights are applied to the value vectors\n",
    "\n",
    "Let's see how this works for a simple tensor filled with ones and note how the resulting values are scaled up to compensate for the dropped out values and to retain balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1603d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tensor before dropout:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "test tensor after dropout:\n",
      "tensor([[2., 2., 2., 2., 0.],\n",
      "        [2., 0., 0., 2., 2.],\n",
      "        [2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 2.],\n",
      "        [2., 0., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # for reproducibility\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "test = torch.ones(5,5)\n",
    "print(\"test tensor before dropout:\")\n",
    "print(test)\n",
    "print(\"test tensor after dropout:\")\n",
    "print(dropout(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc2486",
   "metadata": {},
   "source": [
    "### Self-Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkc_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkc_bias)   # (d_in, d_out); d_in is the embedding vector size\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkc_bias)     # (d_in, d_out)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkc_bias)   # (d_in, d_out)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", \n",
    "                             torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x: input vector (batch_size, num_tokens, d_in)\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)                         # (batch_size, num_tokens, d_in) x (d_in, d_out) = (batch_size, num_tokens, d_out)\n",
    "        queries = self.W_query(x)                    # (batch_size, num_tokens, d_in) x (d_in, d_out) = (batch_size, num_tokens, d_out)\n",
    "        values = self.W_value(x)                     # (batch_size, num_tokens, d_in) x (d_in, d_out) = (batch_size, num_tokens, d_out)   \n",
    "\n",
    "        att_scores = queries @ keys.transpose(1,2)   # (batch_size, num_tokens, d_out) @ (batch_size, d_out, num_tokens) \n",
    "                                                     # = (batch_size, num_tokens, num_tokens)\n",
    "        att_scores.masked_fill_(self.mask.bool(), -torch.inf)\n",
    "\n",
    "        att_weights = torch.softmax(\n",
    "                        att_scores / self.d_out**0.5, dim=-1\n",
    "                      )\n",
    "        att_weights = self.dropout(att_weights)      # (batch_size, num_tokens, num_tokens)\n",
    "\n",
    "        context_vec = att_weights @ values           # (batch_size, num_tokens, num_tokens) @ (batch_size, num_tokens, d_out)\n",
    "                                                     # = (batch_size, num_tokens, d_out)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0af5b",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b19800",
   "metadata": {},
   "source": [
    "We will now move on from using a single attention head to multiple heads.  Each of those has their own KQV tensors: $W_{q1}$, $W_{q2}$, $W_{k1}$, $W_{k2}$, $W_{v1}$, $W_{v2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5ed87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
