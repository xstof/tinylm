{
 "cells": [
  {
   "cell_type": "raw",
   "id": "96a0250e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "author: Christof Claessens\n",
    "date: June 2025\n",
    "title: \"Attention mechanics\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06802ab4",
   "metadata": {},
   "source": [
    "In the previous notebook we prepared and massaged our text into input vectors that encode both tokens as well as their positions.  It's these input vectors that will be handled by the rest of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3708c9",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "\n",
    "\n",
    "Self-Attention is a technique whereby each position in the input sequence can consider the relevancy of each other position in the same sequence when the representation for the sequence is being computed.  (Traditional \"attention\", as opposed to self-attention look at relations between two different sequences, input- and output, as opposed as using a single sequence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1690776",
   "metadata": {},
   "source": [
    "## Simplified Self-Attention\n",
    "\n",
    "Let's take a simple sentence:\n",
    "\"I am learning this\"\n",
    "\n",
    "This is our input sequencen let's call it $x$ with 4 tokens: $x^{(1)}$, $x^{(2)}$, $x^{(3)}$ and $x^{(4)}$.\n",
    "\n",
    "In general if our context length is $T$ then we have $x^{(1)} \\ldots x^{(T)}$\n",
    "\n",
    "Each $x^{(i)}$ is a $d$-dimensional embedding vector representing a token.\n",
    "\n",
    "Now we will calculate a context vector $z^{i}$ for each $x^{i}$.  This vector will contain information from all vectors $x^{(1)} \\ldots x^{(T)}$\n",
    "\n",
    "### Attention weights\n",
    "\n",
    "So, for example, for $x^{(3)}$ we'll calculate a context vector $z^{(3)}$.  We'll call $x^{(3)}$ our \"query\" vector and for this query we'll calculate an attention score for each of the tokens in our sentence: for example $w_{31}$ for the attention score between our query (the 3rd token) and the first token.  In this way we'll have: $w_{31}$, $w_{32}$, $w_{33}$ and $w_{34}$\n",
    "\n",
    "The attention score $w_{31}$ is the dot product from the third token with the first token.  So if:\n",
    "$$ x^{(3)} = [0.2,0.7,0.9] \\text{  (representing the token \"learning\")} $$\n",
    "$$ x^{(1)} = [0.8,0.9,0.3] \\text{  (representing the token \"I\")} $$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$w_{(31)} = x^{(3)} \\cdot x^{(1)} = [(.2*.8) + (.7*.9) + (.9 * .3)] = 1.06 $$\n",
    "\n",
    "We'll do this between our query $x^{(3)}$ and each of our $x^{(1)},x^{(2)},x^{(3)},x^{(4)}$ which result in an attention vector $w_3 = [ w_{(31)},w_{(32)},w_{(33)},w_{(34)} ]$\n",
    "\n",
    "Now in python, using an input vector of our 4 tokens, each with an embedding dimension of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87225baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n"
     ]
    }
   ],
   "source": [
    "#|echo: false\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "inputs = torch.rand((4,3))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb69795",
   "metadata": {},
   "source": [
    "We'll calculate the attention vector for $x^{(3)}$ as our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df259370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3127, 1.1213, 1.5807, 1.3343])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "attention_score_for_x3 = torch.empty((4))\n",
    "for i, xi in enumerate(inputs):\n",
    "    # Calculate the attention score for x3i\n",
    "    attention_score_for_x3[i] = torch.dot(xi, query)\n",
    "\n",
    "print(attention_score_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8fb5d",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "Now that we've calculated the attention vector $w_3$ for query $x^{(3)}$, we have a vector with a number for every token.  These numbers however are not normalized between $0$ and $1$, which is what we'd really like.  For this, we'll use [Cross-Entropy Loss](../theory/cross-entropy-loss.ipynb), which will make sure the numbers in our attention vector add up to one and each are between $0 \\ldots 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d7f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2407, 0.1987, 0.3146, 0.2459])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_for_x3 = torch.softmax(attention_score_for_x3, dim=0)\n",
    "print(attention_weights_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9608f2",
   "metadata": {},
   "source": [
    "### Context Vector\n",
    "\n",
    "So now that we have our normalized attention weights for a single query $x^{(3)}$ we can calculate the full context vector that corresponds to $x^{(3)}$\n",
    "\n",
    "Our attention weights from the previous step were: $[.2, .1, .3, .2]$ or more in general, a vector $[\\alpha _{31}, \\alpha _{32}, \\alpha _{33}, \\alpha _{34}]$\n",
    "\n",
    "To calculate the context vector for $x^{(3)}$ we'll take each $\\alpha _{3i}$ and multiply that by $x^{(i)}$.  Then we'll add up all those vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66414db4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68002bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5165, 0.7774, 0.6536])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "context_vector_for_x3 = torch.zeros((query.shape)) # dimension of query (3 in this case)\n",
    "\n",
    "for i, xi in enumerate(inputs):\n",
    "    context_vector_for_x3 += attention_weights_for_x3[i] * xi # z3\n",
    "\n",
    "print(context_vector_for_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402b7ce",
   "metadata": {},
   "source": [
    "### All context vectors\n",
    "\n",
    "What we did so far is to look at how to calculate a single context vector $z^{(3)}$, for a single token $x^{(3)}$ in our input sequence.  We'll need to make this more scalable and figure out a way to calculate $z$ for all tokens in our input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
