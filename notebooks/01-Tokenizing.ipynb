{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3c8fade8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "author: Christof Claessens\n",
    "date: June 2025\n",
    "title: \"Tokenization\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500ba54",
   "metadata": {},
   "source": [
    "# Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47156c",
   "metadata": {},
   "source": [
    "We'll use an of-the-shelve tokenizer for doing Byte Pair Encoding (BPE).  We'll use `tiktoken` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| output: false\n",
    "%conda install -y tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97027b5a",
   "metadata": {},
   "source": [
    "Let's load a text and tokenize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "filepath = '../data/dracula.txt'\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    return raw_text\n",
    "\n",
    "def tokens_from_text(text: str):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    integers = tokenizer.encode(text)\n",
    "    return integers\n",
    "\n",
    "def text_from_tokens(tokens: list[int]):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    text = tokenizer.decode(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91019cbc",
   "metadata": {},
   "source": [
    "This now allows us to load text and turn it into tokens (each identified by an integer) or the reverse: given a set of tokens, reconstruct the text from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "588b9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Dracula\n",
      " \n",
      "[464, 4935, 20336, 46566, 286, 41142, 198, 220]\n",
      "The Project Gutenberg eBook of Dracula\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def get_sample_text(num_chars:int = 40):\n",
    "    raw_text = load_text(filepath)\n",
    "    return raw_text[:num_chars]\n",
    "\n",
    "sample_text = get_sample_text()\n",
    "print(sample_text)\n",
    "\n",
    "tokens = tokens_from_text(sample_text)\n",
    "print(tokens)\n",
    "\n",
    "text = text_from_tokens(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf016b50",
   "metadata": {},
   "source": [
    "# Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927d857",
   "metadata": {},
   "source": [
    "We'll first make sure to install pytorch: `conda install pytorch cpuonly -c pytorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f13b455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt: str, tokenizer, max_length=16, stride=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt (str): The input text to tokenize and split into sequences.\n",
    "            tokenizer: The tokenizer used to encode the text into token ids.\n",
    "            max_length (int): The context length, i.e., the number of tokens in each input sequence.\n",
    "            stride (int): The step size between the start of consecutive sequences.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length  # context length for each input sequence\n",
    "        self.stride = stride\n",
    "        self.token_ids = self.tokenizer.encode(txt)\n",
    "        self.length = len(self.token_ids)\n",
    "\n",
    "        self.input_ids = []    # list of input tokens, our \"context\" as input to the LLM\n",
    "        self.target_ids = []   # list of target tokens that will need to be predicted, our \"context\" shifted by stride\n",
    "\n",
    "        for i in range(0, len(self.token_ids) - self.max_length):\n",
    "            input_chunk = self.token_ids[i:i + self.max_length]\n",
    "            target_chunk = self.token_ids[i + 1:i + self.max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader(txt:str, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the given text.\n",
    "    Args:\n",
    "        txt (str): The input text to tokenize and split into sequences.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        max_length (int): The context length, i.e., the number of tokens in each input sequence.\n",
    "        stride (int): The step size between the start of consecutive sequences.\n",
    "        shuffle (bool): Whether to shuffle the data at every epoch.\n",
    "        drop_last (bool): Whether to drop the last incomplete batch.\n",
    "        num_workers (int): Number of subprocesses to use for data loading.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = MyDataset(txt, tokenizer, max_length=max_length, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa09778",
   "metadata": {},
   "source": [
    "Let's test our dataloader now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_text:  The Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Dracula\n",
      "\n",
      "Author: Bram Stoker\n",
      "\n",
      "Release date: October 1, 1995 [eBook #345]\n",
      "                Most recently updated: November 12, 2023\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Chuck Greif and the Online Distributed Proofreading Team\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                DRACULA\n",
      "\n",
      "                                  _by_\n",
      "\n",
      "                              Bram Stoker\n",
      "\n",
      "                        [Illustration: colophon]\n",
      "\n",
      "                                NEW YORK\n",
      "\n",
      "              \n",
      "Input IDs: tensor([[220, 220, 220, 220, 220, 220, 220, 220],\n",
      "        [220, 220, 220, 220, 220, 220, 220, 220]])\n",
      "Target IDs: tensor([[220, 220, 220, 220, 220, 220, 220, 220],\n",
      "        [220, 220, 220, 220, 220, 220, 220, 220]])\n",
      "Total batches: 193\n",
      "Batch size: 2\n",
      "Number of workers: 0\n"
     ]
    }
   ],
   "source": [
    "text = get_sample_text(1000)\n",
    "print(\"sample_text: \", text)\n",
    "dataloader = create_dataloader(txt=text, batch_size=2, max_length=8, stride=2, drop_last=False)\n",
    "for batch in dataloader:\n",
    "    input_ids, target_ids = batch\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Target IDs:\", target_ids)\n",
    "    break  # Just show the first batch\n",
    "print(\"Total batches:\", len(dataloader))\n",
    "print(\"Batch size:\", dataloader.batch_size)\n",
    "print(\"Number of workers:\", dataloader.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5383b12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
