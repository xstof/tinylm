{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3c8fade8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "author: Christof Claessens\n",
    "date: June 2025\n",
    "format:\n",
    "  html:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500ba54",
   "metadata": {},
   "source": [
    "# 01 - Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47156c",
   "metadata": {},
   "source": [
    "We'll use an of-the-shelve tokenizer for doing Byte Pair Encoding (BPE).  We'll use `tiktoken` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| output: false\n",
    "%conda install -y tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97027b5a",
   "metadata": {},
   "source": [
    "Let's load a text and tokenize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "816bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "filepath = '../data/dracula.txt'\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    return raw_text\n",
    "\n",
    "def tokens_from_text(text: str):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    integers = tokenizer.encode(text)\n",
    "    return integers\n",
    "\n",
    "def text_from_tokens(tokens: list[int]):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    text = tokenizer.decode(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91019cbc",
   "metadata": {},
   "source": [
    "This now allows us to load text and turn it into tokens (each identified by an integer) or the reverse: given a set of tokens, reconstruct the text from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "588b9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Dracula\n",
      " \n",
      "[464, 4935, 20336, 46566, 286, 41142, 198, 220]\n",
      "The Project Gutenberg eBook of Dracula\n",
      " \n"
     ]
    }
   ],
   "source": [
    "sample_text = load_text(filepath)[:40]\n",
    "print(sample_text)\n",
    "\n",
    "tokens = tokens_from_text(sample_text)\n",
    "print(tokens)\n",
    "\n",
    "text = text_from_tokens(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf016b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927d857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
